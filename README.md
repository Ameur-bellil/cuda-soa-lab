# GPU Matrix Addition Service - Task 1 Implementation

This is a GPU-accelerated matrix addition microservice built with FastAPI and Numba CUDA.

## ğŸš€ Features

- **GPU-accelerated matrix addition** using CUDA kernels via Numba
- **FastAPI REST API** with three endpoints:
  - `GET /health` - Service health check
  - `POST /add` - Matrix addition on GPU
  - `GET /gpu-info` - GPU memory information
- **Input validation** for matrix shape compatibility
- **Performance timing** for GPU operations
- **Docker support** for containerized deployment

## ğŸ“‹ Requirements

- Python 3.10+
- NVIDIA GPU with CUDA support
- NVIDIA drivers installed
- Docker with NVIDIA Container Toolkit (for containerized deployment)

## ğŸ› ï¸ Installation

### Option 1: Virtual Environment (Recommended for Development)

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install fastapi uvicorn numpy numba-cuda prometheus-client python-multipart

# Note: numba-cuda installation requires CUDA toolkit
# For CUDA 12.x:
pip install numba-cuda[cu12]
# For CUDA 13.x:
pip install numba-cuda[cu13]
```

### Option 2: Using requirements.txt

```bash
pip install -r requirements.txt
```

## ğŸ¯ How It Works

### CUDA Kernel Implementation

The service uses a **2D CUDA kernel** for parallel matrix addition:

```python
@cuda.jit
def matrix_add_kernel(a, b, c):
    """Each thread processes one matrix element"""
    i, j = cuda.grid(2)  # Get 2D thread position
    if i < a.shape[0] and j < a.shape[1]:
        c[i, j] = a[i, j] + b[i, j]
```

**Key concepts:**
- **Thread Grid**: The computation is distributed across a 2D grid of threads
- **Block Configuration**: Uses 16Ã—16 threads per block (256 threads)
- **Grid Configuration**: Automatically calculated based on matrix size
- **Memory Management**: Data is transferred to GPU, processed, then copied back

### API Endpoints

#### 1. Health Check
```bash
curl http://localhost:8001/health
```
**Response:**
```json
{"status": "ok"}
```

#### 2. Matrix Addition
```bash
curl -X POST http://localhost:8001/add \
  -F "file_a=@matrix1.npz" \
  -F "file_b=@matrix2.npz"
```
**Response:**
```json
{
  "matrix_shape": [512, 512],
  "elapsed_time": 0.002134,
  "device": "GPU"
}
```

#### 3. GPU Information
```bash
curl http://localhost:8001/gpu-info
```
**Response:**
```json
{
  "gpus": [
    {
      "gpu": "0",
      "memory_used_MB": 312,
      "memory_total_MB": 4096
    }
  ]
}
```

## ğŸ§ª Testing

### Run Validation Tests
```bash
python3 test_service.py
```

This will:
- âœ… Check for sample matrix files
- âœ… Create test matrices
- âœ… Verify CPU computation logic
- âœ… Check installed dependencies
- âœ… Print usage instructions

### Start the Service
```bash
# With virtual environment
source venv/bin/activate
python3 main.py

# Or directly
python3 main.py
```

The service will start on port **8001** (configurable in `main.py`).

### Test Matrix Addition

Using the provided test matrices:
```bash
# Small test matrices (100Ã—100)
curl -X POST http://localhost:8001/add \
  -F "file_a=@test_matrix_a.npz" \
  -F "file_b=@test_matrix_b.npz"

# Larger matrices (512Ã—512)
curl -X POST http://localhost:8001/add \
  -F "file_a=@matrix1.npz" \
  -F "file_b=@matrix2.npz"
```

### Test Error Handling

Test with mismatched matrix shapes:
```bash
curl -X POST http://localhost:8001/add \
  -F "file_a=@test_matrix_a.npz" \
  -F "file_b=@test_matrix_mismatch.npz"
```

Expected: HTTP 400 error with message about shape mismatch.

## ğŸ³ Docker Deployment

### Build the Docker Image
```bash
docker build -t gpu-matrix-service .
```

### Run the Container
```bash
docker run --gpus all -p 8001:8001 gpu-matrix-service
```

### Test the Containerized Service
```bash
curl http://localhost:8001/health
```

## ğŸ“Š Performance Notes

- **Data Transfer Overhead**: For small matrices, CPU might be faster due to GPU memory transfer overhead
- **Optimal Performance**: GPU acceleration shines with larger matrices (>1000Ã—1000)
- **Thread Configuration**: 16Ã—16 block size is optimized for most modern GPUs
- **Memory Usage**: Matrices are temporarily stored on GPU during computation

## ğŸ”§ Configuration

### Change Port Number
Edit the `STUDENT_PORT` variable in `main.py`:
```python
STUDENT_PORT = 8001  # Change to your assigned port
```

### Adjust Thread Block Size
Modify the `threads_per_block` in `gpu_matrix_add()` function:
```python
threads_per_block = (16, 16)  # Default: 256 threads per block
```

## ğŸ“ Project Structure

```
cuda-soa-lab/
â”œâ”€â”€ main.py                    # FastAPI service with CUDA kernel
â”œâ”€â”€ test_service.py           # Validation and testing script
â”œâ”€â”€ matrix1.npz               # Sample matrix 1 (512Ã—512)
â”œâ”€â”€ matrix2.npz               # Sample matrix 2 (512Ã—512)
â”œâ”€â”€ test_matrix_a.npz         # Test matrix A (100Ã—100)
â”œâ”€â”€ test_matrix_b.npz         # Test matrix B (100Ã—100)
â”œâ”€â”€ test_matrix_mismatch.npz  # Mismatch test (50Ã—50)
â”œâ”€â”€ Dockerfile                # Container configuration
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ pyproject.toml           # Project metadata
â””â”€â”€ README.md                # This file
```

## ğŸ“ Learning Points

### CUDA Concepts Demonstrated

1. **Kernel Functions**: GPU functions decorated with `@cuda.jit`
2. **Thread Indexing**: Using `cuda.grid(2)` for 2D indexing
3. **Memory Management**: Explicit data transfer with `cuda.to_device()` and `copy_to_host()`
4. **Grid/Block Configuration**: Calculating optimal thread organization
5. **Boundary Checking**: Preventing out-of-bounds memory access

### When to Use GPU Acceleration

âœ… **Use GPU when:**
- Matrix size is large (>1000Ã—1000)
- Computation per element is non-trivial
- Multiple operations on same data
- Real-time processing needed

âŒ **Avoid GPU when:**
- Matrices are very small
- Single operation with high overhead
- Data transfer dominates computation time

## ğŸ” Troubleshooting

### "No GPU detected" Error
- Verify NVIDIA drivers: `nvidia-smi`
- Check CUDA installation: `nvcc --version`
- Install numba-cuda: `pip install numba-cuda[cu12]`

### "Module not found" Errors
- Activate virtual environment: `source venv/bin/activate`
- Reinstall dependencies: `pip install -r requirements.txt`

### Port Already in Use
- Change `STUDENT_PORT` in `main.py`
- Or kill existing process: `lsof -ti:8001 | xargs kill -9`

## ğŸ“š Next Steps (Lab Tasks)

- âœ… **Task 1**: GPU Matrix Addition Service (COMPLETED)
- â­ï¸ **Task 2**: Add `/gpu-info` endpoint (COMPLETED)
- â­ï¸ **Task 3**: Containerize the application (Dockerfile ready)
- â­ï¸ **Task 4**: Jenkins CI/CD pipeline
- â­ï¸ **Task 5**: Prometheus monitoring and Grafana visualization

## ğŸ‘¨â€ğŸ’» Author

Lab implementation for SOA Course - GPU-Accelerated Microservices

## ğŸ“„ License

Educational use only

